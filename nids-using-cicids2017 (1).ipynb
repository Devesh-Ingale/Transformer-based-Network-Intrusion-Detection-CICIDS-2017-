{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05887048",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-14T10:28:04.995632Z",
     "iopub.status.busy": "2025-09-14T10:28:04.995418Z",
     "iopub.status.idle": "2025-09-14T11:10:36.040905Z",
     "shell.execute_reply": "2025-09-14T11:10:36.039943Z"
    },
    "papermill": {
     "duration": 2551.051367,
     "end_time": "2025-09-14T11:10:36.042281",
     "exception": false,
     "start_time": "2025-09-14T10:28:04.990914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n",
      "Loading friday.csv\n",
      "Loading friday_plus.csv\n",
      "Loading monday.csv\n",
      "Loading monday_plus.csv\n",
      "Loading thursday.csv\n",
      "Loading thursday_plus.csv\n",
      "Loading tuesday.csv\n",
      "Loading tuesday_plus.csv\n",
      "Loading wednesday.csv\n",
      "Loading wednesday_plus.csv\n",
      "Full shape: (4199942, 105)\n",
      "Classes: ['Botnet', 'Botnet - Attempted', 'DDoS', 'DoS GoldenEye', 'DoS GoldenEye - Attempted', 'DoS Hulk', 'DoS Hulk - Attempted', 'DoS Slowhttptest', 'DoS Slowhttptest - Attempted', 'DoS Slowloris', 'DoS Slowloris - Attempted', 'FTP-Patator', 'FTP-Patator - Attempted', 'Heartbleed', 'Infiltration', 'Infiltration - Attempted', 'Infiltration - Portscan', 'Normal', 'Portscan', 'SSH-Patator', 'SSH-Patator - Attempted', 'Web Attack - Brute Force', 'Web Attack - Brute Force - Attempted', 'Web Attack - SQL Injection', 'Web Attack - SQL Injection - Attempted', 'Web Attack - XSS', 'Web Attack - XSS - Attempted']\n",
      "Train shape: (2939959, 101)\n",
      "Using 2 GPUs with DataParallel\n",
      "  step 200/2872  loss=1.4058  lr=1.02e-04  ETA/epoch ~ 4.8 min\n",
      "  step 400/2872  loss=0.8366  lr=1.44e-04  ETA/epoch ~ 4.1 min\n",
      "  step 600/2872  loss=0.5890  lr=1.85e-04  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.4525  lr=2.27e-04  ETA/epoch ~ 3.4 min\n",
      "  step 1000/2872  loss=0.3667  lr=2.69e-04  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.3079  lr=3.00e-04  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.2654  lr=3.00e-04  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.2332  lr=3.00e-04  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.2079  lr=2.99e-04  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.1879  lr=2.99e-04  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.1713  lr=2.98e-04  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.1573  lr=2.98e-04  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.1455  lr=2.97e-04  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.1353  lr=2.96e-04  ETA/epoch ~ 0.1 min\n",
      "Epoch 1/8  TrainLoss: 0.1320  ValMacroF1: 0.6833\n",
      "  step 200/2872  loss=0.0032  lr=2.95e-04  ETA/epoch ~ 4.5 min\n",
      "  step 400/2872  loss=0.0031  lr=2.94e-04  ETA/epoch ~ 4.1 min\n",
      "  step 600/2872  loss=0.0029  lr=2.93e-04  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0028  lr=2.91e-04  ETA/epoch ~ 3.3 min\n",
      "  step 1000/2872  loss=0.0026  lr=2.90e-04  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0029  lr=2.88e-04  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0028  lr=2.87e-04  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0027  lr=2.85e-04  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0026  lr=2.83e-04  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0026  lr=2.81e-04  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0025  lr=2.79e-04  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0024  lr=2.77e-04  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0024  lr=2.75e-04  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0024  lr=2.72e-04  ETA/epoch ~ 0.1 min\n",
      "Epoch 2/8  TrainLoss: 0.0023  ValMacroF1: 0.7500\n",
      "  step 200/2872  loss=0.0034  lr=2.69e-04  ETA/epoch ~ 4.5 min\n",
      "  step 400/2872  loss=0.0032  lr=2.67e-04  ETA/epoch ~ 4.1 min\n",
      "  step 600/2872  loss=0.0027  lr=2.64e-04  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0024  lr=2.61e-04  ETA/epoch ~ 3.3 min\n",
      "  step 1000/2872  loss=0.0023  lr=2.59e-04  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0021  lr=2.56e-04  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0021  lr=2.53e-04  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0023  lr=2.50e-04  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0022  lr=2.47e-04  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0022  lr=2.44e-04  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0021  lr=2.40e-04  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0021  lr=2.37e-04  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0020  lr=2.34e-04  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0020  lr=2.30e-04  ETA/epoch ~ 0.1 min\n",
      "Epoch 3/8  TrainLoss: 0.0020  ValMacroF1: 0.8063\n",
      "  step 200/2872  loss=0.0015  lr=2.26e-04  ETA/epoch ~ 4.4 min\n",
      "  step 400/2872  loss=0.0016  lr=2.22e-04  ETA/epoch ~ 4.0 min\n",
      "  step 600/2872  loss=0.0016  lr=2.19e-04  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0016  lr=2.15e-04  ETA/epoch ~ 3.3 min\n",
      "  step 1000/2872  loss=0.0017  lr=2.12e-04  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0016  lr=2.08e-04  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0016  lr=2.04e-04  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0016  lr=2.00e-04  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0015  lr=1.97e-04  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0015  lr=1.93e-04  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0015  lr=1.89e-04  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0015  lr=1.85e-04  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0015  lr=1.81e-04  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0014  lr=1.78e-04  ETA/epoch ~ 0.1 min\n",
      "Epoch 4/8  TrainLoss: 0.0014  ValMacroF1: 0.8524\n",
      "  step 200/2872  loss=0.0015  lr=1.72e-04  ETA/epoch ~ 4.4 min\n",
      "  step 400/2872  loss=0.0013  lr=1.68e-04  ETA/epoch ~ 4.0 min\n",
      "  step 600/2872  loss=0.0014  lr=1.64e-04  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0013  lr=1.61e-04  ETA/epoch ~ 3.3 min\n",
      "  step 1000/2872  loss=0.0013  lr=1.57e-04  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0012  lr=1.53e-04  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0012  lr=1.49e-04  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0012  lr=1.45e-04  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0012  lr=1.41e-04  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0012  lr=1.37e-04  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0012  lr=1.34e-04  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0012  lr=1.30e-04  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0012  lr=1.26e-04  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0012  lr=1.22e-04  ETA/epoch ~ 0.1 min\n",
      "Epoch 5/8  TrainLoss: 0.0012  ValMacroF1: 0.8633\n",
      "  step 200/2872  loss=0.0011  lr=1.18e-04  ETA/epoch ~ 4.4 min\n",
      "  step 400/2872  loss=0.0010  lr=1.14e-04  ETA/epoch ~ 4.0 min\n",
      "  step 600/2872  loss=0.0012  lr=1.10e-04  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0012  lr=1.07e-04  ETA/epoch ~ 3.3 min\n",
      "  step 1000/2872  loss=0.0012  lr=1.03e-04  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0011  lr=9.99e-05  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0011  lr=9.65e-05  ETA/epoch ~ 2.3 min\n",
      "  step 1600/2872  loss=0.0011  lr=9.32e-05  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0011  lr=8.99e-05  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0011  lr=8.67e-05  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0011  lr=8.36e-05  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0011  lr=8.05e-05  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0011  lr=7.75e-05  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0011  lr=7.46e-05  ETA/epoch ~ 0.1 min\n",
      "Epoch 6/8  TrainLoss: 0.0011  ValMacroF1: 0.8794\n",
      "  step 200/2872  loss=0.0010  lr=7.07e-05  ETA/epoch ~ 4.4 min\n",
      "  step 400/2872  loss=0.0010  lr=6.80e-05  ETA/epoch ~ 4.0 min\n",
      "  step 600/2872  loss=0.0009  lr=6.53e-05  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0009  lr=6.28e-05  ETA/epoch ~ 3.3 min\n",
      "  step 1000/2872  loss=0.0010  lr=6.03e-05  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0010  lr=5.79e-05  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0010  lr=5.55e-05  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0010  lr=5.33e-05  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0010  lr=5.12e-05  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0010  lr=4.91e-05  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0010  lr=4.72e-05  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0010  lr=4.53e-05  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0010  lr=4.36e-05  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0010  lr=4.19e-05  ETA/epoch ~ 0.1 min\n",
      "Epoch 7/8  TrainLoss: 0.0010  ValMacroF1: 0.8859\n",
      "  step 200/2872  loss=0.0010  lr=3.99e-05  ETA/epoch ~ 4.5 min\n",
      "  step 400/2872  loss=0.0010  lr=3.85e-05  ETA/epoch ~ 4.0 min\n",
      "  step 600/2872  loss=0.0010  lr=3.72e-05  ETA/epoch ~ 3.6 min\n",
      "  step 800/2872  loss=0.0010  lr=3.60e-05  ETA/epoch ~ 3.3 min\n",
      "  step 1000/2872  loss=0.0009  lr=3.49e-05  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0009  lr=3.39e-05  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0009  lr=3.30e-05  ETA/epoch ~ 2.3 min\n",
      "  step 1600/2872  loss=0.0009  lr=3.23e-05  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0009  lr=3.16e-05  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0009  lr=3.11e-05  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0009  lr=3.06e-05  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0009  lr=3.03e-05  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0009  lr=3.01e-05  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0009  lr=3.00e-05  ETA/epoch ~ 0.1 min\n",
      "Epoch 8/8  TrainLoss: 0.0009  ValMacroF1: 0.8987\n",
      "Test Macro-F1: 0.8812472839149691\n",
      "                                        precision    recall  f1-score   support\n",
      "\n",
      "                                Botnet       0.77      1.00      0.87       221\n",
      "                    Botnet - Attempted       1.00      1.00      1.00      1220\n",
      "                                  DDoS       1.00      1.00      1.00     28543\n",
      "                         DoS GoldenEye       1.00      1.00      1.00      2270\n",
      "             DoS GoldenEye - Attempted       1.00      0.92      0.96        24\n",
      "                              DoS Hulk       1.00      1.00      1.00     47541\n",
      "                  DoS Hulk - Attempted       0.98      0.99      0.99       174\n",
      "                      DoS Slowhttptest       0.98      1.00      0.99       522\n",
      "          DoS Slowhttptest - Attempted       1.00      1.00      1.00      1011\n",
      "                         DoS Slowloris       1.00      1.00      1.00      1158\n",
      "             DoS Slowloris - Attempted       0.99      1.00      0.99       554\n",
      "                           FTP-Patator       1.00      1.00      1.00      1192\n",
      "               FTP-Patator - Attempted       1.00      1.00      1.00         3\n",
      "                            Heartbleed       0.43      1.00      0.60         3\n",
      "                          Infiltration       0.09      0.73      0.15        11\n",
      "              Infiltration - Attempted       1.00      1.00      1.00        13\n",
      "               Infiltration - Portscan       0.95      1.00      0.98     21530\n",
      "                                Normal       1.00      1.00      1.00    474769\n",
      "                              Portscan       1.00      1.00      1.00     47720\n",
      "                           SSH-Patator       1.00      1.00      1.00       889\n",
      "               SSH-Patator - Attempted       0.89      1.00      0.94         8\n",
      "              Web Attack - Brute Force       1.00      1.00      1.00        22\n",
      "  Web Attack - Brute Force - Attempted       1.00      0.67      0.80       388\n",
      "            Web Attack - SQL Injection       0.67      1.00      0.80         4\n",
      "Web Attack - SQL Injection - Attempted       0.08      1.00      0.15         1\n",
      "                      Web Attack - XSS       0.71      1.00      0.83         5\n",
      "          Web Attack - XSS - Attempted       0.62      0.98      0.76       196\n",
      "\n",
      "                              accuracy                           1.00    629992\n",
      "                             macro avg       0.86      0.97      0.88    629992\n",
      "                          weighted avg       1.00      1.00      1.00    629992\n",
      "\n",
      "\n",
      "Balanced Accuracy: 0.973083\n",
      "Matthews Corrcoef (MCC): 0.994433\n",
      "Cohen's Kappa: 0.994422\n",
      "Macro Precision: 0.856689 | Macro Recall: 0.973083 | Macro F1: 0.881247\n",
      "Saved: test_preds.csv, classification_report.csv, confusion_matrix.npy\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Transformer NIDS for CICIDS-2017 (Multiclass, Rare-class aware)\n",
    "# Stable Kaggle-ready single cell (with extra metrics)\n",
    "# ===============================\n",
    "import os, gc, math, json, warnings, time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, f1_score, confusion_matrix,\n",
    "    balanced_accuracy_score, matthews_corrcoef, cohen_kappa_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# ----------------- 0) Repro -----------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ----------------- 1) Config -----------------\n",
    "DATA_DIR = \"/kaggle/input/cicids-2017\"\n",
    "OUT_DIR  = \"/kaggle/working\"\n",
    "\n",
    "EPOCHS   = 8                # feel free to set 5\n",
    "BATCH    = 1024\n",
    "LR       = 3e-4             # a touch lower for stability\n",
    "WD       = 1e-4\n",
    "D_MODEL  = 96\n",
    "N_HEADS  = 4\n",
    "N_LAYERS = 4\n",
    "FFW      = 256\n",
    "DROPOUT  = 0.20\n",
    "GAMMA    = 1.5              # slightly softer focal\n",
    "CB_BETA  = 0.999            # less extreme class weights\n",
    "WEIGHT_CLIP = 10.0          # cap very large class weights\n",
    "GRAD_CLIP   = 1.0           # clip global grad-norm\n",
    "PATIENCE    = 2             # early stop on macro-F1\n",
    "\n",
    "MERGE_ATTEMPTED = False     # keep full label space by default\n",
    "\n",
    "# ----------------- GPU Info -----------------\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}:\", torch.cuda.get_device_name(i))\n",
    "\n",
    "# ----------------- 2) Load Data -----------------\n",
    "csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".csv\")]\n",
    "assert len(csv_files) > 0, \"No CSVs found in DATA_DIR\"\n",
    "\n",
    "dfs = []\n",
    "for f in sorted(csv_files):\n",
    "    print(\"Loading\", f)\n",
    "    df_part = pd.read_csv(os.path.join(DATA_DIR, f), low_memory=False)\n",
    "    dfs.append(df_part)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Full shape:\", df.shape)\n",
    "\n",
    "# ----------------- 3) Labels -----------------\n",
    "df[\"Label\"] = df[\"Label\"].replace(\"BENIGN\", \"Normal\")\n",
    "if MERGE_ATTEMPTED:\n",
    "    df[\"Label\"] = df[\"Label\"].str.replace(\" - Attempted\", \"\", regex=False)\n",
    "\n",
    "y_le = LabelEncoder()\n",
    "y = y_le.fit_transform(df[\"Label\"])\n",
    "classes = list(y_le.classes_)\n",
    "num_classes = len(classes)\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "# ----------------- 4) Feature Cleaning -----------------\n",
    "drop_cols = [c for c in [\"Flow ID\",\"Src IP\",\"Dst IP\",\"Timestamp\",\"Label\"] if c in df.columns]\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "cat_candidates = [c for c in df.columns if c.lower() in (\"protocol\", \"proto\", \"protocolname\")]\n",
    "cat_cols = [c for c in cat_candidates if c in df.columns]\n",
    "\n",
    "df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "nunq = df.nunique()\n",
    "zero_var = nunq[nunq <= 1].index.tolist()\n",
    "if zero_var:\n",
    "    print(\"Dropping zero-variance cols:\", zero_var)\n",
    "    df = df.drop(columns=zero_var)\n",
    "\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].astype(str)\n",
    "    le_c = LabelEncoder()\n",
    "    df[c] = le_c.fit_transform(df[c])\n",
    "\n",
    "num_cols = [c for c in df.columns if c not in cat_cols]\n",
    "\n",
    "X_cat = df[cat_cols].astype(np.int32).values if cat_cols else None\n",
    "X_num = df[num_cols].astype(np.float32).values if num_cols else None\n",
    "\n",
    "del dfs, df\n",
    "gc.collect()\n",
    "\n",
    "# ----------------- 5) Split + Scale -----------------\n",
    "X_cat_train, X_cat_temp, X_num_train, X_num_temp, y_train, y_temp = train_test_split(\n",
    "    X_cat, X_num, y, test_size=0.30, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_cat_val, X_cat_test, X_num_val, X_num_test, y_val, y_test = train_test_split(\n",
    "    X_cat_temp, X_num_temp, y_temp, test_size=0.50, random_state=SEED, stratify=y_temp\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num_train = scaler.fit_transform(X_num_train)\n",
    "X_num_val   = scaler.transform(X_num_val)\n",
    "X_num_test  = scaler.transform(X_num_test)\n",
    "\n",
    "print(\"Train shape:\", X_num_train.shape)\n",
    "\n",
    "# ----------------- 6) Dataset -----------------\n",
    "class TabularSet(Dataset):\n",
    "    def __init__(self, X_num, X_cat, y):\n",
    "        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.X_cat = torch.tensor(X_cat, dtype=torch.long) if X_cat is not None else None\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_num[idx], (self.X_cat[idx] if self.X_cat is not None else None), self.y[idx]\n",
    "\n",
    "train_ds = TabularSet(X_num_train, X_cat_train, y_train)\n",
    "val_ds   = TabularSet(X_num_val,   X_cat_val,   y_val)\n",
    "test_ds  = TabularSet(X_num_test,  X_cat_test,  y_test)\n",
    "\n",
    "# ----------------- 7) Sampler for Imbalance -----------------\n",
    "class_counts = np.bincount(y_train, minlength=num_classes)\n",
    "inv_freq = 1.0 / np.maximum(class_counts, 1)\n",
    "sample_weights = inv_freq[y_train]\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=torch.tensor(sample_weights, dtype=torch.double),\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# ----------------- 8) Loaders -----------------\n",
    "pin = torch.cuda.is_available()\n",
    "num_workers = 4 if pin else 2\n",
    "dloader_kwargs = dict(num_workers=num_workers, pin_memory=pin, persistent_workers=True, prefetch_factor=4)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=train_sampler, **dloader_kwargs)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, **dloader_kwargs)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, **dloader_kwargs)\n",
    "\n",
    "# ----------------- 9) Model -----------------\n",
    "class CLSPooling(nn.Module):\n",
    "    def forward(self, x): return x[:, 0, :]\n",
    "\n",
    "def get_cat_cardinalities(arr, cols):\n",
    "    if arr is None or not cols: return []\n",
    "    return [int(arr[:, i].max()) + 1 for i in range(arr.shape[1])]\n",
    "\n",
    "cat_cards = get_cat_cardinalities(X_cat_train, cat_cols)\n",
    "\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, n_num, cat_cards, num_classes,\n",
    "                 d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS, ffw=FFW, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.num_tokenizers = nn.ModuleList([nn.Linear(1, d_model) for _ in range(n_num)])\n",
    "        self.cat_embeds = nn.ModuleList([nn.Embedding(c, d_model) for c in cat_cards])\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n",
    "        self.pos = nn.Parameter(torch.randn(1, 1+n_num+len(cat_cards), d_model) * 0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=ffw,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.pool = CLSPooling()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        B = x_num.size(0)\n",
    "        num_tokens = torch.stack([tok(x_num[:, i:i+1]) for i, tok in enumerate(self.num_tokenizers)], dim=1)\n",
    "        if self.cat_embeds:\n",
    "            cat_tokens = torch.stack([emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeds)], dim=1)\n",
    "        else:\n",
    "            cat_tokens = num_tokens.new_zeros((B, 0, num_tokens.size(-1)))\n",
    "        cls = self.cls.expand(B, 1, -1)\n",
    "        tokens = torch.cat([cls, num_tokens, cat_tokens], dim=1)\n",
    "        tokens = tokens + self.pos[:, :tokens.size(1), :]\n",
    "        x = self.encoder(tokens)\n",
    "        x = self.pool(x)\n",
    "        x = self.norm(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# ----------------- 10) Loss: Class-Balanced Focal (device-safe + clipped) -----------------\n",
    "class ClassBalancedFocalLoss(nn.Module):\n",
    "    def __init__(self, samples_per_class, num_classes, beta=CB_BETA, gamma=GAMMA, weight_clip=WEIGHT_CLIP):\n",
    "        super().__init__()\n",
    "        samples = torch.tensor(samples_per_class, dtype=torch.float32)\n",
    "        eff_num = 1.0 - torch.pow(torch.tensor(beta, dtype=torch.float32), samples)\n",
    "        weights = (1.0 - beta) / torch.clamp(eff_num, min=1e-6)\n",
    "        # normalize then clip extreme weights\n",
    "        weights = weights / weights.mean()\n",
    "        if weight_clip is not None:\n",
    "            weights = torch.clamp(weights, max=weight_clip)\n",
    "        self.register_buffer(\"class_weights\", weights)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        w = self.class_weights.to(logits.device, dtype=logits.dtype)\n",
    "        ce = nn.functional.cross_entropy(logits, target, weight=w, reduction='none')\n",
    "        with torch.no_grad():\n",
    "            pt = torch.softmax(logits, dim=1).gather(1, target.view(-1,1)).squeeze(1)\n",
    "        focal = ((1.0 - pt).clamp_(min=1e-6) ** self.gamma) * ce\n",
    "        return focal.mean()\n",
    "\n",
    "# ----------------- 11) Train/Eval -----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TabularTransformer(len(num_cols), cat_cards, num_classes).to(device)\n",
    "\n",
    "# Optional multi-GPU (simple)\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "criterion = ClassBalancedFocalLoss(class_counts, num_classes).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# ---- Warmup -> Cosine scheduler (batch-wise) ----\n",
    "total_steps = EPOCHS * len(train_loader)\n",
    "warmup_steps = max(1000, int(0.05 * total_steps))\n",
    "cosine_steps = max(1, total_steps - warmup_steps)\n",
    "\n",
    "warmup = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.2, total_iters=warmup_steps)\n",
    "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cosine_steps, eta_min=LR * 0.1)\n",
    "sched = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_steps])\n",
    "\n",
    "best_val_macro = -1.0\n",
    "best_path = os.path.join(OUT_DIR, \"best_transformer_cbfl.pt\")\n",
    "no_improve = 0\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        for xb_num, xb_cat, yb in loader:\n",
    "            xb_num, yb = xb_num.to(device), yb.to(device)\n",
    "            xb_cat = xb_cat.to(device) if xb_cat is not None else None\n",
    "            logits = model(xb_num, xb_cat)\n",
    "            preds.append(torch.argmax(logits, 1).cpu().numpy())\n",
    "            labels.append(yb.cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    return f1_score(labels, preds, average=\"macro\"), preds, labels\n",
    "\n",
    "# ----------------- 12) Fit -----------------\n",
    "global_step = 0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    for i, (xb_num, xb_cat, yb) in enumerate(train_loader, 1):\n",
    "        xb_num, yb = xb_num.to(device), yb.to(device)\n",
    "        xb_cat = xb_cat.to(device) if xb_cat is not None else None\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            logits = model(xb_num, xb_cat)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "        # NaN/Inf guard\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f\"Non-finite loss at step {global_step}: {loss.item()}, skipping step.\")\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            global_step += 1\n",
    "            sched.step()\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # clip after unscale\n",
    "        scaler.unscale_(optimizer)\n",
    "        if GRAD_CLIP is not None:\n",
    "            params = model.module.parameters() if isinstance(model, nn.DataParallel) else model.parameters()\n",
    "            torch.nn.utils.clip_grad_norm_(params, GRAD_CLIP)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        sched.step()\n",
    "        total_loss += loss.item()\n",
    "        global_step += 1\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            steps_left = len(train_loader) - i\n",
    "            est = elapsed / i * steps_left\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f\"  step {i}/{len(train_loader)}  loss={total_loss/i:.4f}  lr={current_lr:.2e}  ETA/epoch ~ {est/60:.1f} min\")\n",
    "\n",
    "    val_macro, _, _ = evaluate(val_loader)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}  TrainLoss: {total_loss/len(train_loader):.4f}  ValMacroF1: {val_macro:.4f}\")\n",
    "\n",
    "    if val_macro > best_val_macro + 1e-4:\n",
    "        best_val_macro = val_macro\n",
    "        to_save = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "        torch.save({\n",
    "            \"model\": to_save,\n",
    "            \"classes\": classes,\n",
    "            \"num_cols\": num_cols,\n",
    "            \"cat_cols\": cat_cols,\n",
    "        }, best_path)\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# ----------------- 13) Test -----------------\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "state = ckpt[\"model\"]\n",
    "model.load_state_dict(state, strict=False)\n",
    "\n",
    "test_macro, test_preds, test_labels = evaluate(test_loader)\n",
    "print(\"Test Macro-F1:\", test_macro)\n",
    "print(classification_report(test_labels, test_preds, target_names=classes, zero_division=0))\n",
    "\n",
    "# ---- Extra aggregation metrics (imbalance-robust) ----\n",
    "bal_acc = balanced_accuracy_score(test_labels, test_preds)      # mean recall across classes\n",
    "mcc     = matthews_corrcoef(test_labels, test_preds)            # chance-corrected correlation\n",
    "kappa   = cohen_kappa_score(test_labels, test_preds)            # chance-corrected agreement\n",
    "mp, mr, mf1, _ = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average='macro', zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\nBalanced Accuracy: {bal_acc:.6f}\")\n",
    "print(f\"Matthews Corrcoef (MCC): {mcc:.6f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.6f}\")\n",
    "print(f\"Macro Precision: {mp:.6f} | Macro Recall: {mr:.6f} | Macro F1: {mf1:.6f}\")\n",
    "\n",
    "# Save artifacts\n",
    "pd.DataFrame({\n",
    "    \"true\": [classes[i] for i in test_labels],\n",
    "    \"pred\": [classes[i] for i in test_preds]\n",
    "}).to_csv(os.path.join(OUT_DIR, \"test_preds.csv\"), index=False)\n",
    "\n",
    "rep = classification_report(test_labels, test_preds, target_names=classes, zero_division=0, output_dict=True)\n",
    "pd.DataFrame(rep).to_csv(os.path.join(OUT_DIR, \"classification_report.csv\"))\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds, labels=list(range(num_classes)))\n",
    "np.save(os.path.join(OUT_DIR, \"confusion_matrix.npy\"), cm)\n",
    "print(\"Saved: test_preds.csv, classification_report.csv, confusion_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c68d63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:10:36.059762Z",
     "iopub.status.busy": "2025-09-14T11:10:36.059508Z",
     "iopub.status.idle": "2025-09-14T11:10:36.068371Z",
     "shell.execute_reply": "2025-09-14T11:10:36.067807Z"
    },
    "papermill": {
     "duration": 0.018734,
     "end_time": "2025-09-14T11:10:36.069493",
     "exception": false,
     "start_time": "2025-09-14T11:10:36.050759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Imports & global setup\n",
    "# ===============================\n",
    "import os, gc, math, time, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn bits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, f1_score, confusion_matrix,\n",
    "    balanced_accuracy_score, matthews_corrcoef, cohen_kappa_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Repro\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b24224d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:10:36.086455Z",
     "iopub.status.busy": "2025-09-14T11:10:36.086034Z",
     "iopub.status.idle": "2025-09-14T11:10:36.090202Z",
     "shell.execute_reply": "2025-09-14T11:10:36.089593Z"
    },
    "papermill": {
     "duration": 0.013876,
     "end_time": "2025-09-14T11:10:36.091230",
     "exception": false,
     "start_time": "2025-09-14T11:10:36.077354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd0c6f63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:10:36.107697Z",
     "iopub.status.busy": "2025-09-14T11:10:36.107498Z",
     "iopub.status.idle": "2025-09-14T11:10:36.120101Z",
     "shell.execute_reply": "2025-09-14T11:10:36.119548Z"
    },
    "papermill": {
     "duration": 0.021916,
     "end_time": "2025-09-14T11:10:36.121075",
     "exception": false,
     "start_time": "2025-09-14T11:10:36.099159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATA_DIR: /kaggle/input/cicids-2017\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Config & paths (Kaggle)\n",
    "# ===============================\n",
    "# If you uploaded a custom dataset, adjust DATA_DIR below to match its folder name under /kaggle/input\n",
    "CANDIDATE_DIRS = [\n",
    "    \"/kaggle/input/cicids-2017\",\n",
    "    \"/kaggle/input/cicids2017\",\n",
    "    \"/kaggle/input/cicids-2017-dataset\",\n",
    "    \"/kaggle/input\",  # last-resort scan\n",
    "]\n",
    "\n",
    "def find_data_dir():\n",
    "    target_names = {\"monday.csv\",\"tuesday.csv\",\"wednesday.csv\",\"thursday.csv\",\"friday.csv\"}\n",
    "    for base in CANDIDATE_DIRS:\n",
    "        if not os.path.exists(base): \n",
    "            continue\n",
    "        # exact folder\n",
    "        if any(f.lower().endswith(\".csv\") for f in os.listdir(base)):\n",
    "            names = {f.lower() for f in os.listdir(base)}\n",
    "            if any(n in names for n in target_names):\n",
    "                return base\n",
    "        # scan one level down\n",
    "        for sub in os.listdir(base):\n",
    "            p = os.path.join(base, sub)\n",
    "            if os.path.isdir(p) and any(f.lower().endswith(\".csv\") for f in os.listdir(p)):\n",
    "                names = {f.lower() for f in os.listdir(p)}\n",
    "                if any(n in names for n in target_names):\n",
    "                    return p\n",
    "    raise FileNotFoundError(\"Couldn't find CICIDS-2017 CSVs under /kaggle/input. Please check dataset path.\")\n",
    "\n",
    "DATA_DIR = find_data_dir()\n",
    "OUT_DIR  = \"/kaggle/working\"\n",
    "\n",
    "# Training knobs\n",
    "EPOCHS   = 8\n",
    "BATCH    = 1024\n",
    "LR       = 3e-4\n",
    "WD       = 1e-4\n",
    "D_MODEL  = 96\n",
    "N_HEADS  = 4\n",
    "N_LAYERS = 4\n",
    "FFW      = 256\n",
    "DROPOUT  = 0.20\n",
    "\n",
    "# Class-imbalance loss knobs\n",
    "GAMMA      = 1.5\n",
    "CB_BETA    = 0.999\n",
    "WEIGHT_CLIP= 10.0\n",
    "GRAD_CLIP  = 1.0\n",
    "\n",
    "# Early stop on Val Macro-F1\n",
    "PATIENCE   = 2\n",
    "\n",
    "# Keep full label space (Attempted vs. Non-attempted)\n",
    "MERGE_ATTEMPTED = False\n",
    "\n",
    "print(\"Using DATA_DIR:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2037ced8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:10:36.137715Z",
     "iopub.status.busy": "2025-09-14T11:10:36.137505Z",
     "iopub.status.idle": "2025-09-14T11:11:39.580442Z",
     "shell.execute_reply": "2025-09-14T11:11:39.579723Z"
    },
    "papermill": {
     "duration": 63.452606,
     "end_time": "2025-09-14T11:11:39.581604",
     "exception": false,
     "start_time": "2025-09-14T11:10:36.128998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading friday.csv\n",
      "Loading friday_plus.csv\n",
      "Loading monday.csv\n",
      "Loading monday_plus.csv\n",
      "Loading thursday.csv\n",
      "Loading thursday_plus.csv\n",
      "Loading tuesday.csv\n",
      "Loading tuesday_plus.csv\n",
      "Loading wednesday.csv\n",
      "Loading wednesday_plus.csv\n",
      "Full shape: (4199942, 105)\n",
      "Classes: ['Botnet', 'Botnet - Attempted', 'DDoS', 'DoS GoldenEye', 'DoS GoldenEye - Attempted', 'DoS Hulk', 'DoS Hulk - Attempted', 'DoS Slowhttptest', 'DoS Slowhttptest - Attempted', 'DoS Slowloris', 'DoS Slowloris - Attempted', 'FTP-Patator', 'FTP-Patator - Attempted', 'Heartbleed', 'Infiltration', 'Infiltration - Attempted', 'Infiltration - Portscan', 'Normal', 'Portscan', 'SSH-Patator', 'SSH-Patator - Attempted', 'Web Attack - Brute Force', 'Web Attack - Brute Force - Attempted', 'Web Attack - SQL Injection', 'Web Attack - SQL Injection - Attempted', 'Web Attack - XSS', 'Web Attack - XSS - Attempted']\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Load CSVs\n",
    "# ===============================\n",
    "def load_all_csvs(data_dir):\n",
    "    # usual CICIDS names, but we’ll just take every .csv to be safe\n",
    "    csvs = sorted([f for f in os.listdir(data_dir) if f.lower().endswith(\".csv\")])\n",
    "    if not csvs:\n",
    "        raise FileNotFoundError(\"No CSV files found in DATA_DIR.\")\n",
    "\n",
    "    dfs = []\n",
    "    for f in csvs:\n",
    "        print(\"Loading\", f)\n",
    "        dfp = pd.read_csv(os.path.join(data_dir, f), low_memory=False)\n",
    "        dfs.append(dfp)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = load_all_csvs(DATA_DIR)\n",
    "print(\"Full shape:\", df.shape)\n",
    "\n",
    "# unifying labels\n",
    "df[\"Label\"] = df[\"Label\"].replace(\"BENIGN\", \"Normal\")\n",
    "if MERGE_ATTEMPTED:\n",
    "    df[\"Label\"] = df[\"Label\"].str.replace(\" - Attempted\", \"\", regex=False)\n",
    "\n",
    "# Encode labels\n",
    "y_le = LabelEncoder()\n",
    "y = y_le.fit_transform(df[\"Label\"])\n",
    "classes = list(y_le.classes_)\n",
    "num_classes = len(classes)\n",
    "print(\"Classes:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482f32bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:11:39.599710Z",
     "iopub.status.busy": "2025-09-14T11:11:39.599494Z",
     "iopub.status.idle": "2025-09-14T11:11:39.793608Z",
     "shell.execute_reply": "2025-09-14T11:11:39.792832Z"
    },
    "papermill": {
     "duration": 0.204521,
     "end_time": "2025-09-14T11:11:39.794731",
     "exception": false,
     "start_time": "2025-09-14T11:11:39.590210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Src IP dec</th>\n",
       "      <th>Src Port</th>\n",
       "      <th>Dst IP dec</th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packet</th>\n",
       "      <th>Total Bwd packets</th>\n",
       "      <th>Total Length of Fwd Packet</th>\n",
       "      <th>...</th>\n",
       "      <th>Local_5</th>\n",
       "      <th>Local_6</th>\n",
       "      <th>Local_7</th>\n",
       "      <th>Local_8</th>\n",
       "      <th>Local_9</th>\n",
       "      <th>Local_10</th>\n",
       "      <th>Local_11</th>\n",
       "      <th>Local_12</th>\n",
       "      <th>Local_13</th>\n",
       "      <th>Local_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3232238130</td>\n",
       "      <td>56108</td>\n",
       "      <td>3232238083</td>\n",
       "      <td>3268</td>\n",
       "      <td>6</td>\n",
       "      <td>59:50.3</td>\n",
       "      <td>112740690</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3232238130</td>\n",
       "      <td>42144</td>\n",
       "      <td>3232238083</td>\n",
       "      <td>389</td>\n",
       "      <td>6</td>\n",
       "      <td>59:50.3</td>\n",
       "      <td>112740560</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134610945</td>\n",
       "      <td>0</td>\n",
       "      <td>134219268</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00:31.4</td>\n",
       "      <td>113757377</td>\n",
       "      <td>545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3232238105</td>\n",
       "      <td>5353</td>\n",
       "      <td>3758096635</td>\n",
       "      <td>5353</td>\n",
       "      <td>17</td>\n",
       "      <td>00:42.9</td>\n",
       "      <td>91997219</td>\n",
       "      <td>388</td>\n",
       "      <td>0</td>\n",
       "      <td>37151</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3232238105</td>\n",
       "      <td>123</td>\n",
       "      <td>301796989</td>\n",
       "      <td>123</td>\n",
       "      <td>17</td>\n",
       "      <td>00:42.4</td>\n",
       "      <td>66966070</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>288</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Src IP dec  Src Port  Dst IP dec  Dst Port  Protocol Timestamp  \\\n",
       "0  3232238130     56108  3232238083      3268         6   59:50.3   \n",
       "1  3232238130     42144  3232238083       389         6   59:50.3   \n",
       "2   134610945         0   134219268         0         0   00:31.4   \n",
       "3  3232238105      5353  3758096635      5353        17   00:42.9   \n",
       "4  3232238105       123   301796989       123        17   00:42.4   \n",
       "\n",
       "   Flow Duration  Total Fwd Packet  Total Bwd packets  \\\n",
       "0      112740690                32                 16   \n",
       "1      112740560                32                 16   \n",
       "2      113757377               545                  0   \n",
       "3       91997219               388                  0   \n",
       "4       66966070                 6                  6   \n",
       "\n",
       "   Total Length of Fwd Packet  ...  Local_5  Local_6  Local_7  Local_8  \\\n",
       "0                        6448  ...      NaN      NaN      NaN      NaN   \n",
       "1                        6448  ...      NaN      NaN      NaN      NaN   \n",
       "2                           0  ...      NaN      NaN      NaN      NaN   \n",
       "3                       37151  ...      NaN      NaN      NaN      NaN   \n",
       "4                         288  ...      NaN      NaN      NaN      NaN   \n",
       "\n",
       "   Local_9  Local_10  Local_11  Local_12  Local_13  Local_14  \n",
       "0      NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "1      NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "2      NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "3      NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "4      NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Src IP dec                     int64\n",
      "Src Port                       int64\n",
      "Dst IP dec                     int64\n",
      "Dst Port                       int64\n",
      "Protocol                       int64\n",
      "Timestamp                     object\n",
      "Flow Duration                  int64\n",
      "Total Fwd Packet               int64\n",
      "Total Bwd packets              int64\n",
      "Total Length of Fwd Packet     int64\n",
      "Total Length of Bwd Packet     int64\n",
      "Fwd Packet Length Max          int64\n",
      "dtype: object\n",
      "Unique labels: 27\n"
     ]
    }
   ],
   "source": [
    "display(df.head())\n",
    "print(df.dtypes.head(12))\n",
    "print(\"Unique labels:\", df[\"Label\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a0f854b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:11:39.813646Z",
     "iopub.status.busy": "2025-09-14T11:11:39.813423Z",
     "iopub.status.idle": "2025-09-14T11:11:56.385150Z",
     "shell.execute_reply": "2025-09-14T11:11:56.384399Z"
    },
    "papermill": {
     "duration": 16.582367,
     "end_time": "2025-09-14T11:11:56.386377",
     "exception": false,
     "start_time": "2025-09-14T11:11:39.804010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================\n",
    "# Feature cleaning & encoding\n",
    "# ===============================\n",
    "# Drop obvious non-features if present\n",
    "drop_cols = [c for c in [\"Flow ID\", \"Src IP\", \"Dst IP\", \"Timestamp\", \"Label\"] if c in df.columns]\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "# Identify categorical columns (very few in CICIDS — often Protocol)\n",
    "cat_candidates = [c for c in df.columns if c.lower() in (\"protocol\", \"proto\", \"protocolname\")]\n",
    "cat_cols = [c for c in cat_candidates if c in df.columns]\n",
    "\n",
    "# Fix NaN/Inf, drop zero-variance cols\n",
    "df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "nunq = df.nunique()\n",
    "zero_var = nunq[nunq <= 1].index.tolist()\n",
    "if zero_var:\n",
    "    print(\"Dropping zero-variance cols:\", zero_var)\n",
    "    df = df.drop(columns=zero_var)\n",
    "\n",
    "# Encode categoricals\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    df[c] = le.fit_transform(df[c])\n",
    "\n",
    "num_cols = [c for c in df.columns if c not in cat_cols]\n",
    "\n",
    "# Extract arrays\n",
    "X_cat = df[cat_cols].astype(np.int32).values if cat_cols else None\n",
    "X_num = df[num_cols].astype(np.float32).values if num_cols else None\n",
    "\n",
    "# Free memory\n",
    "del nunq, df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49797ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:11:56.405096Z",
     "iopub.status.busy": "2025-09-14T11:11:56.404865Z",
     "iopub.status.idle": "2025-09-14T11:12:09.023246Z",
     "shell.execute_reply": "2025-09-14T11:12:09.022472Z"
    },
    "papermill": {
     "duration": 12.628791,
     "end_time": "2025-09-14T11:12:09.024435",
     "exception": false,
     "start_time": "2025-09-14T11:11:56.395644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train numerical shape: (2939959, 101)\n",
      "Train categorical shape: (2939959, 1)\n",
      "Classes: 27\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Split & scale\n",
    "# ===============================\n",
    "X_cat_train, X_cat_temp, X_num_train, X_num_temp, y_train, y_temp = train_test_split(\n",
    "    X_cat, X_num, y, test_size=0.30, random_state=SEED, stratify=y\n",
    ")\n",
    "X_cat_val, X_cat_test, X_num_val, X_num_test, y_val, y_test = train_test_split(\n",
    "    X_cat_temp, X_num_temp, y_temp, test_size=0.50, random_state=SEED, stratify=y_temp\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num_train = scaler.fit_transform(X_num_train)\n",
    "X_num_val   = scaler.transform(X_num_val)\n",
    "X_num_test  = scaler.transform(X_num_test)\n",
    "\n",
    "print(\"Train numerical shape:\", X_num_train.shape)\n",
    "if X_cat_train is not None:\n",
    "    print(\"Train categorical shape:\", X_cat_train.shape)\n",
    "print(\"Classes:\", len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b273a749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:12:09.043481Z",
     "iopub.status.busy": "2025-09-14T11:12:09.043279Z",
     "iopub.status.idle": "2025-09-14T11:12:10.239839Z",
     "shell.execute_reply": "2025-09-14T11:12:10.239221Z"
    },
    "papermill": {
     "duration": 1.207296,
     "end_time": "2025-09-14T11:12:10.241159",
     "exception": false,
     "start_time": "2025-09-14T11:12:09.033863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Dataset & DataLoaders\n",
    "# ===============================\n",
    "class TabularSet(Dataset):\n",
    "    def __init__(self, X_num, X_cat, y):\n",
    "        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n",
    "        self.y     = torch.tensor(y, dtype=torch.long)\n",
    "        self.X_cat = torch.tensor(X_cat, dtype=torch.long) if X_cat is not None else None\n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X_num[i], (self.X_cat[i] if self.X_cat is not None else None), self.y[i]\n",
    "\n",
    "train_ds = TabularSet(X_num_train, X_cat_train, y_train)\n",
    "val_ds   = TabularSet(X_num_val,   X_cat_val,   y_val)\n",
    "test_ds  = TabularSet(X_num_test,  X_cat_test,  y_test)\n",
    "\n",
    "# Weighted sampler for imbalance\n",
    "class_counts = np.bincount(y_train, minlength=num_classes)\n",
    "inv_freq = 1.0 / np.maximum(class_counts, 1)\n",
    "sample_weights = inv_freq[y_train]\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=torch.tensor(sample_weights, dtype=torch.double),\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "pin = torch.cuda.is_available()\n",
    "num_workers = 4 if pin else 2\n",
    "dl_args = dict(num_workers=num_workers, pin_memory=pin, persistent_workers=True, prefetch_factor=4)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=train_sampler, **dl_args)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, **dl_args)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, **dl_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "213d1beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:12:10.260337Z",
     "iopub.status.busy": "2025-09-14T11:12:10.260137Z",
     "iopub.status.idle": "2025-09-14T11:12:10.272630Z",
     "shell.execute_reply": "2025-09-14T11:12:10.272117Z"
    },
    "papermill": {
     "duration": 0.02303,
     "end_time": "2025-09-14T11:12:10.273691",
     "exception": false,
     "start_time": "2025-09-14T11:12:10.250661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Model & Loss\n",
    "# ===============================\n",
    "def get_cat_cardinalities(arr):\n",
    "    if arr is None: return []\n",
    "    return [int(arr[:, i].max()) + 1 for i in range(arr.shape[1])]\n",
    "\n",
    "cat_cards = get_cat_cardinalities(X_cat_train)\n",
    "\n",
    "class CLSPooling(nn.Module):\n",
    "    def forward(self, x):  # x: [B, 1+T, D]\n",
    "        return x[:, 0, :]\n",
    "\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, n_num, cat_cards, num_classes,\n",
    "                 d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS, ffw=FFW, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.num_tokenizers = nn.ModuleList([nn.Linear(1, d_model) for _ in range(n_num)])\n",
    "        self.cat_embeds = nn.ModuleList([nn.Embedding(c, d_model) for c in cat_cards])\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n",
    "        self.pos = nn.Parameter(torch.randn(1, 1+n_num+len(cat_cards), d_model) * 0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=ffw,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.pool = CLSPooling()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        B = x_num.size(0)\n",
    "        num_tokens = torch.stack([tok(x_num[:, i:i+1]) for i, tok in enumerate(self.num_tokenizers)], dim=1)\n",
    "        if self.cat_embeds:\n",
    "            cat_tokens = torch.stack([emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeds)], dim=1)\n",
    "        else:\n",
    "            cat_tokens = num_tokens.new_zeros((B, 0, num_tokens.size(-1)))\n",
    "        cls = self.cls.expand(B, 1, -1)\n",
    "        x = torch.cat([cls, num_tokens, cat_tokens], dim=1)\n",
    "        x = x + self.pos[:, :x.size(1), :]\n",
    "        x = self.encoder(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.norm(x)\n",
    "        return self.head(x)\n",
    "\n",
    "class ClassBalancedFocalLoss(nn.Module):\n",
    "    def __init__(self, samples_per_class, num_classes, beta=CB_BETA, gamma=GAMMA, weight_clip=WEIGHT_CLIP):\n",
    "        super().__init__()\n",
    "        samples = torch.tensor(samples_per_class, dtype=torch.float32)\n",
    "        eff_num = 1.0 - torch.pow(torch.tensor(beta, dtype=torch.float32), samples)\n",
    "        weights = (1.0 - beta) / torch.clamp(eff_num, min=1e-6)\n",
    "        weights = weights / weights.mean()\n",
    "        if weight_clip is not None:\n",
    "            weights = torch.clamp(weights, max=weight_clip)\n",
    "        self.register_buffer(\"class_weights\", weights)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        w = self.class_weights.to(logits.device, dtype=logits.dtype)\n",
    "        ce = nn.functional.cross_entropy(logits, target, weight=w, reduction='none')\n",
    "        with torch.no_grad():\n",
    "            pt = torch.softmax(logits, dim=1).gather(1, target.view(-1,1)).squeeze(1)\n",
    "        focal = ((1.0 - pt).clamp_(min=1e-6) ** self.gamma) * ce\n",
    "        return focal.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc0f491",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:12:10.292131Z",
     "iopub.status.busy": "2025-09-14T11:12:10.291913Z",
     "iopub.status.idle": "2025-09-14T11:52:26.703664Z",
     "shell.execute_reply": "2025-09-14T11:52:26.702386Z"
    },
    "papermill": {
     "duration": 2416.446375,
     "end_time": "2025-09-14T11:52:26.728736",
     "exception": false,
     "start_time": "2025-09-14T11:12:10.282361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs with DataParallel\n",
      "  step 200/2872  loss=1.4058  lr=1.02e-04  ETA/epoch ~ 4.7 min\n",
      "  step 400/2872  loss=0.8366  lr=1.44e-04  ETA/epoch ~ 4.2 min\n",
      "  step 600/2872  loss=0.5890  lr=1.85e-04  ETA/epoch ~ 3.8 min\n",
      "  step 800/2872  loss=0.4525  lr=2.27e-04  ETA/epoch ~ 3.4 min\n",
      "  step 1000/2872  loss=0.3667  lr=2.69e-04  ETA/epoch ~ 3.1 min\n",
      "  step 1200/2872  loss=0.3079  lr=3.00e-04  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.2654  lr=3.00e-04  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.2332  lr=3.00e-04  ETA/epoch ~ 2.1 min\n",
      "  step 1800/2872  loss=0.2079  lr=2.99e-04  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.1879  lr=2.99e-04  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.1713  lr=2.98e-04  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.1573  lr=2.98e-04  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.1455  lr=2.97e-04  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.1353  lr=2.96e-04  ETA/epoch ~ 0.1 min\n",
      "Epoch 1/8  TrainLoss: 0.1320  ValMacroF1: 0.6833\n",
      "  step 200/2872  loss=0.0032  lr=2.95e-04  ETA/epoch ~ 4.5 min\n",
      "  step 400/2872  loss=0.0031  lr=2.94e-04  ETA/epoch ~ 4.1 min\n",
      "  step 600/2872  loss=0.0029  lr=2.93e-04  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0028  lr=2.91e-04  ETA/epoch ~ 3.4 min\n",
      "  step 1000/2872  loss=0.0026  lr=2.90e-04  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0029  lr=2.88e-04  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0028  lr=2.87e-04  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0027  lr=2.85e-04  ETA/epoch ~ 2.1 min\n",
      "  step 1800/2872  loss=0.0026  lr=2.83e-04  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0026  lr=2.81e-04  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0025  lr=2.79e-04  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0024  lr=2.77e-04  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0024  lr=2.75e-04  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0024  lr=2.72e-04  ETA/epoch ~ 0.1 min\n",
      "Epoch 2/8  TrainLoss: 0.0023  ValMacroF1: 0.7500\n",
      "  step 200/2872  loss=0.0034  lr=2.69e-04  ETA/epoch ~ 4.5 min\n",
      "  step 400/2872  loss=0.0032  lr=2.67e-04  ETA/epoch ~ 4.1 min\n",
      "  step 600/2872  loss=0.0027  lr=2.64e-04  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0024  lr=2.61e-04  ETA/epoch ~ 3.4 min\n",
      "  step 1000/2872  loss=0.0023  lr=2.59e-04  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0021  lr=2.56e-04  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0021  lr=2.53e-04  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0023  lr=2.50e-04  ETA/epoch ~ 2.1 min\n",
      "  step 1800/2872  loss=0.0022  lr=2.47e-04  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0022  lr=2.44e-04  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0021  lr=2.40e-04  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0021  lr=2.37e-04  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0020  lr=2.34e-04  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0020  lr=2.30e-04  ETA/epoch ~ 0.1 min\n",
      "Epoch 3/8  TrainLoss: 0.0020  ValMacroF1: 0.8063\n",
      "  step 200/2872  loss=0.0015  lr=2.26e-04  ETA/epoch ~ 4.5 min\n",
      "  step 400/2872  loss=0.0016  lr=2.22e-04  ETA/epoch ~ 4.1 min\n",
      "  step 600/2872  loss=0.0016  lr=2.19e-04  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0016  lr=2.15e-04  ETA/epoch ~ 3.4 min\n",
      "  step 1000/2872  loss=0.0017  lr=2.12e-04  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0016  lr=2.08e-04  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0016  lr=2.04e-04  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0016  lr=2.00e-04  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0015  lr=1.97e-04  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0015  lr=1.93e-04  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0015  lr=1.89e-04  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0015  lr=1.85e-04  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0015  lr=1.81e-04  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0014  lr=1.78e-04  ETA/epoch ~ 0.1 min\n",
      "Epoch 4/8  TrainLoss: 0.0014  ValMacroF1: 0.8524\n",
      "  step 200/2872  loss=0.0015  lr=1.72e-04  ETA/epoch ~ 4.4 min\n",
      "  step 400/2872  loss=0.0013  lr=1.68e-04  ETA/epoch ~ 4.0 min\n",
      "  step 600/2872  loss=0.0014  lr=1.64e-04  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0013  lr=1.61e-04  ETA/epoch ~ 3.3 min\n",
      "  step 1000/2872  loss=0.0013  lr=1.57e-04  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0012  lr=1.53e-04  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0012  lr=1.49e-04  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0012  lr=1.45e-04  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0012  lr=1.41e-04  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0012  lr=1.37e-04  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0012  lr=1.34e-04  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0012  lr=1.30e-04  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0012  lr=1.26e-04  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0012  lr=1.22e-04  ETA/epoch ~ 0.1 min\n",
      "Epoch 5/8  TrainLoss: 0.0012  ValMacroF1: 0.8633\n",
      "  step 200/2872  loss=0.0011  lr=1.18e-04  ETA/epoch ~ 4.5 min\n",
      "  step 400/2872  loss=0.0010  lr=1.14e-04  ETA/epoch ~ 4.1 min\n",
      "  step 600/2872  loss=0.0012  lr=1.10e-04  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0012  lr=1.07e-04  ETA/epoch ~ 3.4 min\n",
      "  step 1000/2872  loss=0.0012  lr=1.03e-04  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0011  lr=9.99e-05  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0011  lr=9.65e-05  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0011  lr=9.32e-05  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0011  lr=8.99e-05  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0011  lr=8.67e-05  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0011  lr=8.36e-05  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0011  lr=8.05e-05  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0011  lr=7.75e-05  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0011  lr=7.46e-05  ETA/epoch ~ 0.1 min\n",
      "Epoch 6/8  TrainLoss: 0.0011  ValMacroF1: 0.8794\n",
      "  step 200/2872  loss=0.0010  lr=7.07e-05  ETA/epoch ~ 4.5 min\n",
      "  step 400/2872  loss=0.0010  lr=6.80e-05  ETA/epoch ~ 4.0 min\n",
      "  step 600/2872  loss=0.0009  lr=6.53e-05  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0009  lr=6.28e-05  ETA/epoch ~ 3.4 min\n",
      "  step 1000/2872  loss=0.0010  lr=6.03e-05  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0010  lr=5.79e-05  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0010  lr=5.55e-05  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0010  lr=5.33e-05  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0010  lr=5.12e-05  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0010  lr=4.91e-05  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0010  lr=4.72e-05  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0010  lr=4.53e-05  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0010  lr=4.36e-05  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0010  lr=4.19e-05  ETA/epoch ~ 0.1 min\n",
      "Epoch 7/8  TrainLoss: 0.0010  ValMacroF1: 0.8859\n",
      "  step 200/2872  loss=0.0010  lr=3.99e-05  ETA/epoch ~ 4.5 min\n",
      "  step 400/2872  loss=0.0010  lr=3.85e-05  ETA/epoch ~ 4.0 min\n",
      "  step 600/2872  loss=0.0010  lr=3.72e-05  ETA/epoch ~ 3.7 min\n",
      "  step 800/2872  loss=0.0010  lr=3.60e-05  ETA/epoch ~ 3.3 min\n",
      "  step 1000/2872  loss=0.0009  lr=3.49e-05  ETA/epoch ~ 3.0 min\n",
      "  step 1200/2872  loss=0.0009  lr=3.39e-05  ETA/epoch ~ 2.7 min\n",
      "  step 1400/2872  loss=0.0009  lr=3.30e-05  ETA/epoch ~ 2.4 min\n",
      "  step 1600/2872  loss=0.0009  lr=3.23e-05  ETA/epoch ~ 2.0 min\n",
      "  step 1800/2872  loss=0.0009  lr=3.16e-05  ETA/epoch ~ 1.7 min\n",
      "  step 2000/2872  loss=0.0009  lr=3.11e-05  ETA/epoch ~ 1.4 min\n",
      "  step 2200/2872  loss=0.0009  lr=3.06e-05  ETA/epoch ~ 1.1 min\n",
      "  step 2400/2872  loss=0.0009  lr=3.03e-05  ETA/epoch ~ 0.8 min\n",
      "  step 2600/2872  loss=0.0009  lr=3.01e-05  ETA/epoch ~ 0.4 min\n",
      "  step 2800/2872  loss=0.0009  lr=3.00e-05  ETA/epoch ~ 0.1 min\n",
      "Epoch 8/8  TrainLoss: 0.0009  ValMacroF1: 0.8987\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Training\n",
    "# ===============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TabularTransformer(len(num_cols), cat_cards, num_classes).to(device)\n",
    "\n",
    "# Optional: DataParallel if multiple GPUs\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "criterion = ClassBalancedFocalLoss(class_counts, num_classes).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# Scheduler: warmup then cosine\n",
    "total_steps = EPOCHS * len(train_loader)\n",
    "warmup_steps = max(1000, int(0.05 * total_steps))\n",
    "cosine_steps = max(1, total_steps - warmup_steps)\n",
    "warmup = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.2, total_iters=warmup_steps)\n",
    "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cosine_steps, eta_min=LR * 0.1)\n",
    "sched = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_steps])\n",
    "\n",
    "best_val_macro = -1.0\n",
    "no_improve = 0\n",
    "best_path = os.path.join(OUT_DIR, \"best_transformer_cbfl.pt\")\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        for xb_num, xb_cat, yb in loader:\n",
    "            xb_num, yb = xb_num.to(device), yb.to(device)\n",
    "            xb_cat = xb_cat.to(device) if xb_cat is not None else None\n",
    "            logits = model(xb_num, xb_cat)\n",
    "            preds.append(torch.argmax(logits, 1).cpu().numpy())\n",
    "            labels.append(yb.cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    return f1_score(labels, preds, average=\"macro\"), preds, labels\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    for i, (xb_num, xb_cat, yb) in enumerate(train_loader, 1):\n",
    "        xb_num, yb = xb_num.to(device), yb.to(device)\n",
    "        xb_cat = xb_cat.to(device) if xb_cat is not None else None\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            logits = model(xb_num, xb_cat)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "        # Guard against NaN/Inf\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f\"Non-finite loss at step {global_step}: {loss.item()}, skipping.\")\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            global_step += 1\n",
    "            sched.step()\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        if GRAD_CLIP is not None:\n",
    "            params = model.module.parameters() if isinstance(model, nn.DataParallel) else model.parameters()\n",
    "            torch.nn.utils.clip_grad_norm_(params, GRAD_CLIP)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        sched.step()\n",
    "        total_loss += loss.item()\n",
    "        global_step += 1\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            steps_left = len(train_loader) - i\n",
    "            est = elapsed / i * steps_left\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f\"  step {i}/{len(train_loader)}  loss={total_loss/i:.4f}  lr={current_lr:.2e}  ETA/epoch ~ {est/60:.1f} min\")\n",
    "\n",
    "    val_macro, _, _ = evaluate(val_loader)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}  TrainLoss: {total_loss/len(train_loader):.4f}  ValMacroF1: {val_macro:.4f}\")\n",
    "\n",
    "    if val_macro > best_val_macro + 1e-4:\n",
    "        best_val_macro = val_macro\n",
    "        to_save = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "        torch.save({\n",
    "            \"model\": to_save,\n",
    "            \"classes\": classes,\n",
    "            \"num_cols\": num_cols,\n",
    "            \"cat_cols\": cat_cols,\n",
    "        }, best_path)\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bba6177e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:52:26.760924Z",
     "iopub.status.busy": "2025-09-14T11:52:26.760654Z",
     "iopub.status.idle": "2025-09-14T11:52:54.734746Z",
     "shell.execute_reply": "2025-09-14T11:52:54.733907Z"
    },
    "papermill": {
     "duration": 27.989607,
     "end_time": "2025-09-14T11:52:54.735995",
     "exception": false,
     "start_time": "2025-09-14T11:52:26.746388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Macro-F1: 0.8812472839149691\n",
      "                                        precision    recall  f1-score   support\n",
      "\n",
      "                                Botnet       0.77      1.00      0.87       221\n",
      "                    Botnet - Attempted       1.00      1.00      1.00      1220\n",
      "                                  DDoS       1.00      1.00      1.00     28543\n",
      "                         DoS GoldenEye       1.00      1.00      1.00      2270\n",
      "             DoS GoldenEye - Attempted       1.00      0.92      0.96        24\n",
      "                              DoS Hulk       1.00      1.00      1.00     47541\n",
      "                  DoS Hulk - Attempted       0.98      0.99      0.99       174\n",
      "                      DoS Slowhttptest       0.98      1.00      0.99       522\n",
      "          DoS Slowhttptest - Attempted       1.00      1.00      1.00      1011\n",
      "                         DoS Slowloris       1.00      1.00      1.00      1158\n",
      "             DoS Slowloris - Attempted       0.99      1.00      0.99       554\n",
      "                           FTP-Patator       1.00      1.00      1.00      1192\n",
      "               FTP-Patator - Attempted       1.00      1.00      1.00         3\n",
      "                            Heartbleed       0.43      1.00      0.60         3\n",
      "                          Infiltration       0.09      0.73      0.15        11\n",
      "              Infiltration - Attempted       1.00      1.00      1.00        13\n",
      "               Infiltration - Portscan       0.95      1.00      0.98     21530\n",
      "                                Normal       1.00      1.00      1.00    474769\n",
      "                              Portscan       1.00      1.00      1.00     47720\n",
      "                           SSH-Patator       1.00      1.00      1.00       889\n",
      "               SSH-Patator - Attempted       0.89      1.00      0.94         8\n",
      "              Web Attack - Brute Force       1.00      1.00      1.00        22\n",
      "  Web Attack - Brute Force - Attempted       1.00      0.67      0.80       388\n",
      "            Web Attack - SQL Injection       0.67      1.00      0.80         4\n",
      "Web Attack - SQL Injection - Attempted       0.08      1.00      0.15         1\n",
      "                      Web Attack - XSS       0.71      1.00      0.83         5\n",
      "          Web Attack - XSS - Attempted       0.62      0.98      0.76       196\n",
      "\n",
      "                              accuracy                           1.00    629992\n",
      "                             macro avg       0.86      0.97      0.88    629992\n",
      "                          weighted avg       1.00      1.00      1.00    629992\n",
      "\n",
      "\n",
      "Balanced Accuracy: 0.973083\n",
      "Matthews Corrcoef (MCC): 0.994433\n",
      "Cohen's Kappa: 0.994422\n",
      "Macro Precision: 0.856689 | Macro Recall: 0.973083 | Macro F1: 0.881247\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Evaluation on test set\n",
    "# ===============================\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "state = ckpt[\"model\"]\n",
    "model.load_state_dict(state, strict=False)\n",
    "\n",
    "test_macro, test_preds, test_labels = evaluate(test_loader)\n",
    "print(\"Test Macro-F1:\", test_macro)\n",
    "\n",
    "print(classification_report(test_labels, test_preds, target_names=classes, zero_division=0))\n",
    "\n",
    "# Extra imbalance-robust metrics\n",
    "bal_acc = balanced_accuracy_score(test_labels, test_preds)\n",
    "mcc     = matthews_corrcoef(test_labels, test_preds)\n",
    "kappa   = cohen_kappa_score(test_labels, test_preds)\n",
    "mp, mr, mf1, _ = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average=\"macro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\nBalanced Accuracy: {bal_acc:.6f}\")\n",
    "print(f\"Matthews Corrcoef (MCC): {mcc:.6f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.6f}\")\n",
    "print(f\"Macro Precision: {mp:.6f} | Macro Recall: {mr:.6f} | Macro F1: {mf1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e9a7b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T11:52:54.765056Z",
     "iopub.status.busy": "2025-09-14T11:52:54.764363Z",
     "iopub.status.idle": "2025-09-14T11:52:56.781311Z",
     "shell.execute_reply": "2025-09-14T11:52:56.780574Z"
    },
    "papermill": {
     "duration": 2.0325,
     "end_time": "2025-09-14T11:52:56.782512",
     "exception": false,
     "start_time": "2025-09-14T11:52:54.750012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: test_preds.csv, classification_report.csv, confusion_matrix.npy\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Save artifacts to /kaggle/working\n",
    "# ===============================\n",
    "pred_df = pd.DataFrame({\n",
    "    \"true\": [classes[i] for i in test_labels],\n",
    "    \"pred\": [classes[i] for i in test_preds]\n",
    "})\n",
    "pred_df.to_csv(os.path.join(OUT_DIR, \"test_preds.csv\"), index=False)\n",
    "\n",
    "report = classification_report(test_labels, test_preds, target_names=classes, zero_division=0, output_dict=True)\n",
    "pd.DataFrame(report).to_csv(os.path.join(OUT_DIR, \"classification_report.csv\"))\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds, labels=list(range(len(classes))))\n",
    "np.save(os.path.join(OUT_DIR, \"confusion_matrix.npy\"), cm)\n",
    "\n",
    "print(\"Saved: test_preds.csv, classification_report.csv, confusion_matrix.npy\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5405420,
     "sourceId": 8977525,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5098.84042,
   "end_time": "2025-09-14T11:52:59.820502",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-14T10:28:00.980082",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
